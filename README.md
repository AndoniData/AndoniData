<h1 align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&duration=3000&pause=1000&color=00F5D4&center=true&vCenter=true&width=650&lines=Hi+%F0%9F%91%8B+I'm+Andoni+Lagos;Data+Specialist+%E2%86%92+Data+Engineer;Data+Engineering+%7C+Web+Scraping+%7C+Automation" alt="Typing SVG" />
</h1>

---

## ğŸš€ My Data Engineer Journey

Iâ€™m on an exciting path evolving from a **Data Specialist** into a proficient **Data Engineer**, focused on building scalable, maintainable data infrastructure that empowers businesses.

Hereâ€™s a summary of my journey â€” a combination of skills, projects, and learning steps shaping my transformation:

---

### ğŸ” Step 1: Strong Data Foundations  
- Mastering data extraction and cleaning with **Python**, **SQL**, and **data wrangling libraries** like `Pandas` & `NumPy`.  
- Handling diverse file formats: `.csv`, `.json`, `.parquet`, and web data (HTML, CSS).  
- Strengthening my stack with Python (Pandas, NumPy, PyArrow), focusing on building clean, reliable, and automated data pipelines.  
- Data Modeling with **SQL** views and CTEs management.  

---

### âš™ï¸ Step 2: Building Automated Pipelines  
- Developing ETL pipelines to ingest, transform, and load data efficiently.  
- Implementing process control with **logging**, **timers**, and **error handling** to ensure pipeline reliability.  
- Working with databases like **PostgreSQL** for structured storage and retrieval.  

---

### â˜ï¸ Step 3: Transitioning to Cloud-Native Solutions (GCP Edition)  
- Expanding expertise in **Google Cloud Platform (GCP)** services:  
  - **Cloud Storage (GCS)** for data lakes and file management  
  - **Cloud Dataflow** and **Dataprep** for ETL and large-scale transformations  
  - **Cloud Functions** for serverless event-driven workflows  
  - **Cloud SQL** / **BigQuery** for relational and analytical workloads  
- Containerizing workflows with **Docker** and orchestrating DAGs with **Cloud Composer (Airflow)**.  

---

### ğŸ”® Step 4: Exploring Advanced Tools & Emerging Tech  
- Leveraging distributed compute engines like **Apache Spark** on **Dataproc** for batch and streaming workloads.  
- Integrating **Large Language Models (LLMs)** and APIs to enhance data workflows, insights generation, and prompt engineering use cases.  
- Designing real-time pipelines for **financial market, retail, and real estate analytics**, combining cloud-native compute with BI dashboards.  

---

### ğŸ“‚ Projects Highlighting My Journey  

| Project | Description | Tech |
|---------|-------------|------|
| [**Retail Data Scraper - Falabella**](#) | Dynamic data extraction combining `Selenium` & `Requests` to scrape product and pricing data | `Python` `Selenium` `Requests` |
| [**ETL Pipeline for Analytics in Real State**](#) | Automated data ingestion and transformation pipeline feeding BI dashboards | `Cloud Composer` `Pandas` `SQL` |
| [**Bitcoin Real-Time ETL**](#) | (In Pause) Streaming pipeline for real-time cryptocurrency data analysis | `Kafka` `Spark Streaming` `GCP Pub/Sub` `Cloud Storage` |
| [**Mercado Publico - Chile**](#) | (In Progress) Streaming pipeline for public offers by State of Chile | `Kafka` `GCP` `Telegram BOT` `Python` |

---

## ğŸ’» Tech Stack Summary  

ğŸ”§ **Tools:** Python Â· SQL Â· Git Â· Linux (Ubuntu/Debian) Â· PostgreSQL Â· Logging & Timers Â· Jira Service Management  
â˜ **Cloud:** GCP (Cloud Storage Â· BigQuery Â· Cloud Functions Â· Dataproc Â· Cloud Composer)  
ğŸ“Š **BI Tools:** Power BI Â· Apache Superset Â· Metabase Â· Streamlit  
ğŸ“š **Frameworks:** Pandas Â· NumPy Â· DuckDB Â· Selenium Â· Apache Spark Â· PyArrow  
ğŸ›  **Technologies:** DDL Â· Data Modeling  
ğŸ¢ **Systems:** SAP S/4HANA Â· SAP Datasphere *(legacy)*  
ğŸ§  **Skills:** OOP Â· Conceptual & Business Logical Data Modeling Â· Legacy Systems  
ğŸŒ± **Learning:** Docker Â· Airflow Â· LLM Tools & API Integration Â· GCP Data Environment  
ğŸ“¦ **File Types:** .parquet Â· .csv Â· .json Â· HTML Â· CSS  
ğŸ’» **Local Dev:** `uv` Â· `venv`  

---

## ğŸ“« Connect & Collaborate

<p align="center">
  <a href="https://www.linkedin.com/in/andoni-lagos/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white"/>
  </a>
  <a href="https://github.com/AndoniData" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  </a>
  <a href="#" target="_blank">
    <img src="https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white"/>
  </a>
</p>

---

â­ _Always open to collaborating on data engineering, pipeline automation, and scalable cloud-native data solutions!_

<!---
AndoniData/AndoniData is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->

