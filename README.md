<h1 align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&duration=3000&pause=1000&color=00F5D4&center=true&vCenter=true&width=650&lines=Hi+%F0%9F%91%8B+I'm+Andoni+Lagos;Data+Specialist+%E2%86%92+Data+Engineer;Data+Engineering+%7C+Web+Scraping+%7C+Automation" alt="Typing SVG" />
</h1>

---

## 🚀 My Data Engineer Journey

I’m on an exciting path evolving from a **Data Specialist** into a proficient **Data Engineer**, focused on building scalable, maintainable data infrastructure that empowers businesses.

Here’s a summary of my journey — a combination of skills, projects, and learning steps shaping my transformation:

---

### 🔍 Step 1: Strong Data Foundations  
- Mastering data extraction and cleaning with **Python**, **SQL**, and **data wrangling libraries** like `Pandas` & `NumPy`.  
- Handling diverse file formats: `.csv`, `.json`, `.parquet`, and web data (HTML, CSS).  
- Strengthening my stack with Python (Pandas, NumPy, PyArrow), focusing on building clean, reliable, and automated data pipelines.
- Data Modeling with **SQL** views and CTEs management
---

### ⚙️ Step 2: Building Automated Pipelines  
- Developing ETL pipelines to ingest, transform, and load data efficiently.  
- Implementing process control with **logging**, **timers**, and **error handling** to ensure pipeline reliability.  
- Working with databases like **PostgreSQL** for structured storage and retrieval.

---

### ☁️ Step 3: Transitioning to Cloud-Native Solutions  
- Currently expanding expertise in **AWS** services:  
  - **S3** for data lakes and storage  
  - **Glue** for ETL, data cataloging, and transformations  
  - **Lambda** for serverless automation and event-driven workflows  
  - **RDS** for relational database management  
- Containerizing workflows with **Docker** and orchestrating jobs with **Apache Airflow** (DAGs).  

---

### 🔮 Step 4: Exploring Advanced Tools & Emerging Tech  
- Leveraging compute engines like **Apache Spark**, **AWS EMR**, and **AWS Lambda** to process and scale data pipelines efficiently.  
- Integrating **Large Language Models (LLMs)** and APIs to enhance data workflows, insights generation, and support prompt engineering use cases.  
- Preparing to implement real-time data pipelines for **financial market, retail, and real estate analytics**, combining cloud-native compute with BI dashboards.  


---

### 📂 Projects Highlighting My Journey  

| Project | Description | Tech |
|---------|-------------|------|
| [**Retail Data Scraper - Falabella**](#) | Dynamic data extraction combining `Selenium` & `Requests` to scrape product and pricing data | `Python` `Selenium` `Requests` |
| [**ETL Pipeline for Analytics in Real State**](#) | (In Progress) Automated data ingestion and transformation pipeline feeding BI dashboards | `Airflow` `Pandas` `SQL` |
| [**Bitcoin Real-Time ETL**](#) | (In Progress) Streaming pipeline for real-time cryptocurrency data analysis | `Kafka` `Spark Streaming` `AWS Kinesis` `S3` |


---

## 💻 Tech Stack Summary  

🔧 **Tools:** Python · SQL · Git · Linux (Ubuntu/Debian) · PostgreSQL · Logging & Timers · Jira Service Management  
☁ **Cloud:** AWS (S3 · Glue · Lambda · RDS · EMR) *(transitioning)*  
📊 **BI Tools:** Power BI · Apache Superset · Metabase · Streamlit   
📚 **Frameworks:** Pandas · NumPy · DuckDB · Selenium · Apache Spark · PyArrow  
🛠 **Technologies:** DDL · Data Modeling  
🏢 **Systems:** SAP S/4HANA · SAP Datasphere *(legacy)*  
🧠 **Skills:** OOP · Conceptual & Business Logical Data Modeling · Legacy Systems  
🌱 **Learning:** Docker · Airflow · LLM Tools & API Integration · AWS Environment  
📦 **File Types:** .parquet · .csv · .json · HTML · CSS  
💻 **Local Dev:** `uv` · `venv`  

---

## 📫 Connect & Collaborate

<p align="center">
  <a href="https://www.linkedin.com/in/andoni-lagos/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white"/>
  </a>
  <a href="https://github.com/AndoniData" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  </a>
  <a href="#" target="_blank">
    <img src="https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white"/>
  </a>
</p>

---

⭐ _Always open to collaborating on data engineering, pipeline automation, and scalable data solutions!_

<!---
AndoniData/AndoniData is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
