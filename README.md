<h1 align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&duration=3000&pause=1000&color=00F5D4&center=true&vCenter=true&width=650&lines=Hi+%F0%9F%91%8B+I'm+Andoni+Lagos;Data+Specialist+%E2%86%92+Data+Engineer;Data+Engineering+%7C+Web+Scraping+%7C+Automation" alt="Typing SVG" />
</h1>

---

## ğŸš€ My Data Engineer Journey

Iâ€™m on an exciting path evolving from a **Data Specialist** into a proficient **Data Engineer**, focused on building scalable, maintainable data infrastructure that empowers businesses.

Hereâ€™s a summary of my journey â€” a combination of skills, projects, and learning steps shaping my transformation:

---

### ğŸ” Step 1: Strong Data Foundations  
- Mastering data extraction and cleaning with **Python**, **SQL**, and **data wrangling libraries** like `Pandas` & `NumPy`.  
- Handling diverse file formats: `.csv`, `.json`, `.parquet`, and web data (HTML, CSS).  
- Strengthening my stack with Python (Pandas, NumPy, PyArrow), focusing on building clean, reliable, and automated data pipelines.
- Data Modeling with **SQL** views and CTEs management
---

### âš™ï¸ Step 2: Building Automated Pipelines  
- Developing ETL pipelines to ingest, transform, and load data efficiently.  
- Implementing process control with **logging**, **timers**, and **error handling** to ensure pipeline reliability.  
- Working with databases like **PostgreSQL** for structured storage and retrieval.

---

### â˜ï¸ Step 3: Transitioning to Cloud-Native Solutions  
- Currently expanding expertise in **AWS** services:  
  - **S3** for data lakes and storage  
  - **Glue** for ETL, data cataloging, and transformations  
  - **Lambda** for serverless automation and event-driven workflows  
  - **RDS** for relational database management  
- Containerizing workflows with **Docker** and orchestrating jobs with **Apache Airflow** (DAGs).  

---

### ğŸ”® Step 4: Exploring Advanced Tools & Emerging Tech  
- Leveraging compute engines like **Apache Spark**, **AWS EMR**, and **AWS Lambda** to process and scale data pipelines efficiently.  
- Integrating **Large Language Models (LLMs)** and APIs to enhance data workflows, insights generation, and support prompt engineering use cases.  
- Preparing to implement real-time data pipelines for **financial market, retail, and real estate analytics**, combining cloud-native compute with BI dashboards.  


---

### ğŸ“‚ Projects Highlighting My Journey  

| Project | Description | Tech |
|---------|-------------|------|
| [**Retail Data Scraper - Falabella**](#) | Dynamic data extraction combining `Selenium` & `Requests` to scrape product and pricing data | `Python` `Selenium` `Requests` |
| [**ETL Pipeline for Analytics in Real State**](#) | (In Progress) Automated data ingestion and transformation pipeline feeding BI dashboards | `Airflow` `Pandas` `SQL` |
| [**Bitcoin Real-Time ETL**](#) | (In Progress) Streaming pipeline for real-time cryptocurrency data analysis | `Kafka` `Spark Streaming` `AWS Kinesis` `S3` |


---

## ğŸ’» Tech Stack Summary  

ğŸ”§ **Tools:** Python Â· SQL Â· Git Â· Linux (Ubuntu/Debian) Â· PostgreSQL Â· Logging & Timers Â· Jira Service Management  
â˜ **Cloud:** AWS (S3 Â· Glue Â· Lambda Â· RDS Â· EMR) *(transitioning)*  
ğŸ“Š **BI Tools:** Power BI Â· Apache Superset Â· Metabase Â· Streamlit   
ğŸ“š **Frameworks:** Pandas Â· NumPy Â· DuckDB Â· Selenium Â· Apache Spark Â· PyArrow  
ğŸ›  **Technologies:** DDL Â· Data Modeling  
ğŸ¢ **Systems:** SAP S/4HANA Â· SAP Datasphere *(legacy)*  
ğŸ§  **Skills:** OOP Â· Conceptual & Business Logical Data Modeling Â· Legacy Systems  
ğŸŒ± **Learning:** Docker Â· Airflow Â· LLM Tools & API Integration Â· AWS Environment  
ğŸ“¦ **File Types:** .parquet Â· .csv Â· .json Â· HTML Â· CSS  
ğŸ’» **Local Dev:** `uv` Â· `venv`  

---

## ğŸ“« Connect & Collaborate

<p align="center">
  <a href="https://www.linkedin.com/in/andoni-lagos/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white"/>
  </a>
  <a href="https://github.com/AndoniData" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  </a>
  <a href="#" target="_blank">
    <img src="https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white"/>
  </a>
</p>

---

â­ _Always open to collaborating on data engineering, pipeline automation, and scalable data solutions!_

<!---
AndoniData/AndoniData is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
