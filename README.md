<h1 align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&duration=3000&pause=1000&color=00F5D4&center=true&vCenter=true&width=650&lines=Hi+%F0%9F%91%8B+I'm+Andoni+Lagos;Data+Specialist+%E2%86%92+Data+Engineer;Data+Engineering+%7C+Web+Scraping+%7C+Automation" alt="Typing SVG" />
</h1>

---

## 🚀 My Data Engineer Journey

I’m on an exciting path evolving from a **Data Specialist** into a proficient **Data Engineer**, focused on building scalable, maintainable data infrastructure that empowers businesses.

Here’s a summary of my journey — a combination of skills, projects, and learning steps shaping my transformation:

---

### 🔍 Step 1: Strong Data Foundations  
- Mastering data extraction and cleaning with **Python**, **SQL**, and **data wrangling libraries** like `Pandas` & `NumPy`.  
- Handling diverse file formats: `.csv`, `.json`, `.parquet`, and web data (HTML, CSS).  
- Building automation and stablish bases with **Selenium**, **Requests**, **CCFI**, **Asyncio** and more! for reliable web scraping projects.
- Data Modeling with **SQL** based on DDL views and CTEs management
---

### ⚙️ Step 2: Building Automated Pipelines  
- Developing ETL pipelines to ingest, transform, and load data efficiently.  
- Implementing process control with **logging**, **timers**, and **error handling** to ensure pipeline reliability.  
- Working with databases like **PostgreSQL** for structured storage and retrieval.

---

### ☁️ Step 3: Transitioning to Cloud-Native Solutions  
- Currently expanding expertise in **Google Cloud Platform** services:  
  - **BigQuery** for scalable analytics  
  - **Google Cloud Storage (GCS)** for data lakes  
  - **Dataproc** for managed Spark clusters  
- Containerizing workflows using **Docker** and orchestrating jobs with **Apache Airflow** (DAGs).

---

### 🔮 Step 4: Exploring Advanced Tools & Emerging Tech  
- Experimenting with frameworks like **Playwright**, **NoDriver** and undectable browser like camoufox for faster scraping automation.  
- Integrating **Large Language Models (LLMs)** and APIs to enhance data workflows and insights.  
- Preparing to implement real-time data pipelines for cryptocurrency, retail and real state analytics.

---

### 📂 Projects Highlighting My Journey  

| Project | Description | Tech |
|---------|-------------|------|
| [**Retail Data Scraper - Falabella**](#) | Dynamic data extraction combining `Selenium` & `Requests` to scrape product and pricing data | `Python` `Selenium` `Requests` |
| [**ETL Pipeline for Analytics in Real State**](#) | (In Progress) Automated data ingestion and transformation pipeline feeding BI dashboards | `Airflow` `Pandas` `SQL` |
| [**Bitcoin Real-Time ETL**](#) | (In Progress) Streaming pipeline for real-time cryptocurrency data analysis | `Kafka` `Spark Streaming` `BigQuery` |

---

## 💻 Tech Stack Summary

🔧 **Tools:** Python · SQL · Git · Linux (Ubuntu/Debian) · PostgreSQL · Logging & Timers · Jira Service Management
☁ **Cloud:** GCP (BigQuery · GCS · Dataproc) *(transitioning)*  
📊 **BI Tools:** Power BI · Apache Superset · Looker Studio  
📚 **Frameworks:** Pandas · NumPy · DuckDB · Selenium · Apache Spark · PyArrow  
🛠 **Technologies:** DDL · Data Modeling  
🏢 **Systems:** SAP S/4HANA · SAP Datasphere *(legacy)*  
🧠 **Skills:** OOP · Conceptual & business logical Data Modeling · Legacy Systems  
🌱 **Learning:** Playwright · Docker · Airflow · LLM Tools & API Integration  · GCP Environment
📦 **File Types:** .parquet · .csv · .json · HTML · CSS  
💻 **Local Dev:** `uv` · `venv`  

---

## 📫 Connect & Collaborate

<p align="center">
  <a href="https://www.linkedin.com/in/andoni-lagos/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white"/>
  </a>
  <a href="https://github.com/AndoniData" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  </a>
  <a href="#" target="_blank">
    <img src="https://img.shields.io/badge/Portfolio-000000?style=for-the-badge&logo=About.me&logoColor=white"/>
  </a>
</p>

---

⭐ _Always open to collaborating on data engineering, pipeline automation, and scalable data solutions!_

<!---
AndoniData/AndoniData is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
